{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c2f51a0",
   "metadata": {},
   "source": [
    "# Feature Selection Techniques for Fraud Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116dddbc",
   "metadata": {},
   "source": [
    "In this video, we will walk through a comprehensive process of applying the following feature engineering techniques:\n",
    "\n",
    "1. Filter Methods \n",
    "2. Wrapper Methods\n",
    "3. Embedded Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1e06c",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e15b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f456d4",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "553bc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into pandas DataFrame\n",
    "df = pd.read_csv('C:/Users/Amarkou/Documents/Ecourse/creditcard.csv')\n",
    "# Select the first 30,000 rows of the DataFrame\n",
    "df = df.head(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce407f43",
   "metadata": {},
   "source": [
    "## Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82090f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from features\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839885a",
   "metadata": {},
   "source": [
    "# 1. Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984e6d6",
   "metadata": {},
   "source": [
    "### 1.1 Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cfd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation coefficients\n",
    "cor_list = []\n",
    "for i in X_train.columns.tolist():\n",
    "    cor = np.corrcoef(X_train[i], y_train)[0, 1]\n",
    "    cor_list.append(cor)\n",
    "    \n",
    "# Replace NaN with 0\n",
    "cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "\n",
    "# Feature name\n",
    "cor_feature = X_train.iloc[:,np.argsort(np.abs(cor_list))[-10:]].columns.tolist()\n",
    "\n",
    "# Feature selection\n",
    "X_train_filtered = X_train[cor_feature]\n",
    "X_test_filtered = X_test[cor_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5660f8",
   "metadata": {},
   "source": [
    "### 1.2 Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5947029",
   "metadata": {},
   "source": [
    "Variance Threshold is a simple baseline approach to feature selection. It removes all features which variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcb93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Variance Threshold\n",
    "selector = VarianceThreshold(threshold=0.5)\n",
    "selector.fit_transform(X_train)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "X_train_low_variance = X_train.iloc[:,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8324a1",
   "metadata": {},
   "source": [
    "### 1.3  Chi-Squared Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d4814",
   "metadata": {},
   "source": [
    "The Chi-Square statistic is commonly used for testing relationships between categorical variables. In feature selection, we aim to select the features which are highly dependent on the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e49f8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Chi-Squared Test\n",
    "chi_selector = SelectKBest(chi2, k=10)\n",
    "chi_selector.fit_transform(abs(X_train), y_train)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = chi_selector.get_support(indices=True)\n",
    "X_train_chi2 = X_train.iloc[:,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee10499",
   "metadata": {},
   "source": [
    "### 1.4 Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19240d4a",
   "metadata": {},
   "source": [
    "Mutual information measures the information that X and Y share: It measures how much knowing one of these variables reduces uncertainty about the other. For example, if X and Y are independent, then knowing X does not give any information about Y and vice versa, so their mutual information is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e30b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Mutual Information\n",
    "mi_selector = SelectKBest(mutual_info_classif, k=10)\n",
    "mi_selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = mi_selector.get_support(indices=True)\n",
    "X_train_mi = X_train.iloc[:,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56b26f",
   "metadata": {},
   "source": [
    "# 2. Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f02c66",
   "metadata": {},
   "source": [
    "### 2.1 Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf3731",
   "metadata": {},
   "source": [
    "Recursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached.\n",
    "\n",
    "We're starting by initializing the RFE model using logistic regression as the estimator. Then, we fit the model to our training data and transform our data to only include the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fea758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMarkou\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AMarkou\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize an RFE model using the logistic regression estimator\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(estimator=model, n_features_to_select=10, step=1)\n",
    "\n",
    "# Fit the model\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89145ea3",
   "metadata": {},
   "source": [
    "### 2.2 Sequential Feature Selection (SFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846324cd",
   "metadata": {},
   "source": [
    "Sequential Feature Selection (SFS) is a type of greedy search algorithm that is used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.\n",
    "\n",
    "We're starting by initializing the SFS model using KNN as the estimator. Then, we fit the model to our training data and transform our data to only include the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81932ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize an SFS model using the KNN estimator\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "sfs = SequentialFeatureSelector(knn, n_features_to_select=10)\n",
    "\n",
    "# Fit the model\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train_sfs = sfs.transform(X_train)\n",
    "X_test_sfs = sfs.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f031d0",
   "metadata": {},
   "source": [
    "### 2.3 Genetic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e0bc0",
   "metadata": {},
   "source": [
    "Genetic Algorithms are search based algorithms based on the concepts of natural selection and genetics.\n",
    "\n",
    "Unfortunately, there's no direct implementation for GA in scikit-learn, but there are several packages, such as DEAP, that can be used for this.\n",
    "\n",
    "Here, we define a custom evaluation function that uses a basic neural network classifier. Then we define the various genetic algorithm functions and parameters, including creating the initial population, defining the evaluation, mating, mutation, and selection methods, and then run the algorithm for a defined number of generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5fd45e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg     \tstd       \tmin     \tmax     \n",
      "0  \t50    \t0.996833\t0.00125698\t0.993333\t0.999167\n",
      "1  \t36    \t0.997483\t0.00185719\t0.987833\t0.999167\n",
      "2  \t27    \t0.998393\t0.000803299\t0.996167\t0.999167\n",
      "3  \t24    \t0.998783\t0.000551009\t0.996333\t0.999333\n",
      "4  \t27    \t0.998863\t0.000430362\t0.997667\t0.9995  \n",
      "5  \t26    \t0.998733\t0.00106771 \t0.993167\t0.9995  \n",
      "6  \t32    \t0.998983\t0.000316667\t0.998   \t0.9995  \n",
      "7  \t27    \t0.998993\t0.00041628 \t0.997333\t0.9995  \n",
      "8  \t24    \t0.99891 \t0.000850627\t0.9955  \t0.9995  \n",
      "9  \t25    \t0.99911 \t0.000414474\t0.998   \t0.9995  \n",
      "10 \t26    \t0.999147\t0.000336056\t0.998167\t0.9995  \n",
      "11 \t35    \t0.998967\t0.000422953\t0.998167\t0.9995  \n",
      "12 \t27    \t0.99904 \t0.000541151\t0.996   \t0.9995  \n",
      "13 \t24    \t0.998993\t0.000645463\t0.996   \t0.9995  \n",
      "14 \t25    \t0.999067\t0.000521749\t0.996833\t0.9995  \n",
      "15 \t33    \t0.998977\t0.00045948 \t0.997333\t0.9995  \n",
      "16 \t34    \t0.998613\t0.00179178 \t0.986667\t0.9995  \n",
      "17 \t37    \t0.998957\t0.000337984\t0.998167\t0.9995  \n",
      "18 \t34    \t0.99883 \t0.000661219\t0.995167\t0.9995  \n",
      "19 \t25    \t0.99897 \t0.00049068 \t0.997167\t0.9995  \n",
      "20 \t35    \t0.99894 \t0.000366303\t0.998   \t0.999667\n",
      "21 \t24    \t0.999007\t0.000452106\t0.997   \t0.999667\n",
      "22 \t31    \t0.999047\t0.000344867\t0.998333\t0.999667\n",
      "23 \t33    \t0.998997\t0.000374893\t0.998   \t0.999667\n",
      "24 \t19    \t0.999173\t0.000374106\t0.998167\t0.999667\n",
      "25 \t29    \t0.999143\t0.000393008\t0.998   \t0.999667\n",
      "26 \t32    \t0.9991  \t0.00034641 \t0.998167\t0.999667\n",
      "27 \t35    \t0.999037\t0.000383391\t0.998167\t0.999667\n",
      "28 \t30    \t0.999103\t0.000358841\t0.998167\t0.999667\n",
      "29 \t23    \t0.999127\t0.000347307\t0.998167\t0.999667\n",
      "30 \t25    \t0.99906 \t0.00049862 \t0.997   \t0.999667\n",
      "31 \t34    \t0.99888 \t0.00136953 \t0.989667\t0.999667\n",
      "32 \t25    \t0.999127\t0.000376475\t0.998167\t0.999667\n",
      "33 \t25    \t0.99912 \t0.000452204\t0.997833\t0.999667\n",
      "34 \t37    \t0.998923\t0.00060938 \t0.996167\t0.999667\n",
      "35 \t33    \t0.998973\t0.000498174\t0.997   \t0.999667\n",
      "36 \t25    \t0.99902 \t0.000383898\t0.998167\t0.999667\n",
      "37 \t28    \t0.999033\t0.000377124\t0.998167\t0.999667\n",
      "38 \t26    \t0.99906 \t0.0005393  \t0.996   \t0.999667\n",
      "39 \t33    \t0.99903 \t0.000676338\t0.994833\t0.999667\n",
      "40 \t31    \t0.999087\t0.000377477\t0.998   \t0.999667\n"
     ]
    }
   ],
   "source": [
    "# Install DEAP if not already installed\n",
    "# !pip install deap\n",
    "\n",
    "from deap import creator, base, tools, algorithms\n",
    "from sklearn import neural_network\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    mask = list(map(bool, individual))\n",
    "    return (neural_network.MLPClassifier().fit(X_train.iloc[:, mask], y_train).score(X_test.iloc[:, mask], y_test),)\n",
    "\n",
    "# Define the genetic algorithm functions and parameters\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", np.random.choice, 2, p=[0.1, 0.9])\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, len(X_train.columns))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Run the genetic algorithm\n",
    "pop = toolbox.population(n=50)\n",
    "hof = tools.HallOfFame(1)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=40, \n",
    "                               stats=stats, halloffame=hof, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11945f86",
   "metadata": {},
   "source": [
    " We're selecting the top features according to the optimal solution (individual) found by the genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a86a4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features\n",
    "top_features = [X_train.columns[i] for i in range(len(hof[0])) if hof[0][i] == 1]\n",
    "X_train_ga = X_train[top_features]\n",
    "X_test_ga = X_test[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637f779",
   "metadata": {},
   "source": [
    "# 3. Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35087e06",
   "metadata": {},
   "source": [
    "### 3.1 Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71e31e",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) adds \"absolute value of magnitude\" of coefficient as penalty term to the loss function. This can lead to the reduction of some coefficients to zero, effectively performing feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff763d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Initialize the LassoCV model\n",
    "lasso = LassoCV(cv=5)\n",
    "\n",
    "# Fit the model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Select features\n",
    "lasso_mask = lasso.coef_ != 0\n",
    "X_train_lasso = X_train.loc[:, lasso_mask]\n",
    "X_test_lasso = X_test.loc[:, lasso_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63e1aa",
   "metadata": {},
   "source": [
    "### 3.2 Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ffaea",
   "metadata": {},
   "source": [
    "Elastic Net is a middle ground between Lasso Regression and Ridge Regression. It includes the penalties of both models, effectively shrinking some coefficients and setting some to zero for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32d82820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Initialize the ElasticNetCV model\n",
    "elastic = ElasticNetCV(cv=5)\n",
    "\n",
    "# Fit the model\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "# Select features\n",
    "elastic_mask = elastic.coef_ != 0\n",
    "X_train_elastic = X_train.loc[:, elastic_mask]\n",
    "X_test_elastic = X_test.loc[:, elastic_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7d59d",
   "metadata": {},
   "source": [
    "### 3.3 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0704319",
   "metadata": {},
   "source": [
    "Decision Trees are able to rank features based on their importance by the amount that each feature decrease the weighted impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e568dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the DecisionTreeClassifier model\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Select features based on feature importances\n",
    "tree_mask = tree.feature_importances_ > 0.01\n",
    "X_train_tree = X_train.loc[:, tree_mask]\n",
    "X_test_tree = X_test.loc[:, tree_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebddebe",
   "metadata": {},
   "source": [
    "### 3.4 Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2f49b",
   "metadata": {},
   "source": [
    "Similarly to Decision Trees, Random Forests are also able to rank features based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "616da72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the RandomForestClassifier model\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Fit the model\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# Select features based on feature importances\n",
    "forest_mask = forest.feature_importances_ > 0.01\n",
    "X_train_forest = X_train.loc[:, forest_mask]\n",
    "X_test_forest = X_test.loc[:, forest_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3580506",
   "metadata": {},
   "source": [
    "### 3.5 Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae2215",
   "metadata": {},
   "source": [
    "Just like Decision Trees and Random Forests, Gradient Boosting models are also able to rank features based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a4ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the GradientBoostingClassifier model\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# Fit the model\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Select features based on feature importances\n",
    "gbc_mask = gbc.feature_importances_ > 0.01\n",
    "X_train_gbc = X_train.loc[:, gbc_mask]\n",
    "X_test_gbc = X_test.loc[:, gbc_mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
